{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "275c4dfb-c91e-454a-8cad-02df1d3048ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "\n",
    "# !pip install tensorflow scikit-learn pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "1050c2a3-9345-4a07-8d95-68e21bd7d338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection                  import train_test_split\n",
    "from tensorflow.keras.preprocessing.text      import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence  import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "4b924ad6-2922-445f-a8be-001a7777abb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations\n",
    "\n",
    "records = 2000\n",
    "params  = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "4f6daec2-d208-433b-baa0-a9ea664ce28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 1000) (2000, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('dataset/browsing_history_dataset.csv')\n",
    "\n",
    "data = data[:records]\n",
    "\n",
    "# Combine the query and past search results into a single text feature\n",
    "data['combined_text'] = data['query'] + ' ' + data['past_searches']\n",
    "\n",
    "# # Tokenize the text\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(data['combined_text'])\n",
    "\n",
    "# print(tokenizer, dir(tokenizer))\n",
    "# Assuming `data` is a DataFrame with columns 'query', 'likes', and 'priority_percentage'\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(data['query'] + ' ' + data['past_searches'])\n",
    "\n",
    "# Convert text to sequences\n",
    "query_sequences = tokenizer.texts_to_sequences(data['query'])\n",
    "likes_sequences = tokenizer.texts_to_sequences(data['past_searches'])\n",
    "\n",
    "# Pad sequences\n",
    "query_padded = pad_sequences(query_sequences, padding='post', maxlen = params)\n",
    "likes_padded = pad_sequences(likes_sequences, padding='post', maxlen = params)\n",
    "\n",
    "\n",
    "print(query_padded.shape, likes_padded.shape)\n",
    "# print(data[['query', 'past_searches', 'priority_score']], query_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "a31d4cb0-8965-4940-aafd-65dc42941391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Deep tonight indeed war per.'"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_sequences\n",
    "data['query'][0]\n",
    "# tokenizer.texts_to_sequences([data['query'][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "eeab91d4-5773-4c86-b032-17c39dcd9940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 1000) (2000, 1000)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Concatenate, SimpleRNN, Dense\n",
    "\n",
    "# Define the model architecture\n",
    "query_input = Input(shape=(None,), dtype='int32')\n",
    "likes_input = Input(shape=(None,), dtype='int32')\n",
    "\n",
    "# Embedding layers for both inputs\n",
    "query_embedding = Embedding(input_dim=params, output_dim=128)(query_input)\n",
    "likes_embedding = Embedding(input_dim=params, output_dim=128)(likes_input)\n",
    "\n",
    "# Concatenate the embeddings\n",
    "merged = Concatenate()([query_embedding, likes_embedding])\n",
    "\n",
    "# Simple RNN layer\n",
    "rnn_layer = SimpleRNN(128)(merged)\n",
    "\n",
    "# Dense layer for regression\n",
    "output = Dense(1)(rnn_layer)\n",
    "\n",
    "# Compile the model\n",
    "model = Model(inputs=[query_input, likes_input], outputs=output)\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "print(query_padded.shape, likes_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "60630994-da06-4bb5-b793-7c2787218ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 287ms/step - accuracy: 0.0047 - loss: 0.1148\n",
      "Epoch 2/2\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 283ms/step - accuracy: 0.0036 - loss: 0.0956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2f9f43200>"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "model.fit([query_padded, likes_padded], data['priority_score'], epochs=2, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "de21bc41-5d30-488d-ac4e-e3686c9c41fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# saving\n",
    "with open('../models/tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# loading\n",
    "with open('../models/tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer1 = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "a9272cc4-a173-446c-80ca-70ada1e4747d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 566]]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['x ray physics indeed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "a5d1c7b9-2c49-4f40-a03c-b28c0ac5bcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 88ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.418385  ],\n",
       "       [0.41838494],\n",
       "       [0.41838503],\n",
       "       ...,\n",
       "       [0.41838482],\n",
       "       [0.41838533],\n",
       "       [0.41838485]], dtype=float32)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([query_padded, likes_padded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "ef5c4b94-a5d8-49ff-a83e-88768017ccc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('hello.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "dae7f438-17f6-4140-8892-9c439cecf0f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<OOV>': 1,\n",
       " 'bag': 2,\n",
       " 'home': 3,\n",
       " 'air': 4,\n",
       " 'drop': 5,\n",
       " 'or': 6,\n",
       " 'east': 7,\n",
       " 'stay': 8,\n",
       " 'lot': 9,\n",
       " 'civil': 10,\n",
       " 'fall': 11,\n",
       " 'like': 12,\n",
       " 'at': 13,\n",
       " 'real': 14,\n",
       " 'his': 15,\n",
       " 'oil': 16,\n",
       " 'five': 17,\n",
       " 'rest': 18,\n",
       " 'now': 19,\n",
       " 'center': 20,\n",
       " 'tell': 21,\n",
       " 'idea': 22,\n",
       " 'a': 23,\n",
       " 'alone': 24,\n",
       " 'cup': 25,\n",
       " 'then': 26,\n",
       " 'join': 27,\n",
       " 'four': 28,\n",
       " 'miss': 29,\n",
       " 'tree': 30,\n",
       " 'want': 31,\n",
       " 'accept': 32,\n",
       " 'race': 33,\n",
       " 'order': 34,\n",
       " 'too': 35,\n",
       " 'kid': 36,\n",
       " 'all': 37,\n",
       " 'road': 38,\n",
       " 'man': 39,\n",
       " 'set': 40,\n",
       " 'task': 41,\n",
       " 'whom': 42,\n",
       " 'that': 43,\n",
       " 'describe': 44,\n",
       " 'push': 45,\n",
       " 'last': 46,\n",
       " 'list': 47,\n",
       " 'ten': 48,\n",
       " 'agree': 49,\n",
       " 'short': 50,\n",
       " 'break': 51,\n",
       " 'game': 52,\n",
       " 'hold': 53,\n",
       " 'yet': 54,\n",
       " 'have': 55,\n",
       " 'admit': 56,\n",
       " 'focus': 57,\n",
       " 'if': 58,\n",
       " 'sense': 59,\n",
       " 'go': 60,\n",
       " 'win': 61,\n",
       " 'let': 62,\n",
       " 'way': 63,\n",
       " 'next': 64,\n",
       " 'south': 65,\n",
       " 'really': 66,\n",
       " 'try': 67,\n",
       " 'free': 68,\n",
       " 'reach': 69,\n",
       " 'wrong': 70,\n",
       " 'unit': 71,\n",
       " 'few': 72,\n",
       " 'on': 73,\n",
       " 'my': 74,\n",
       " 'wish': 75,\n",
       " 'pm': 76,\n",
       " 'speech': 77,\n",
       " 'show': 78,\n",
       " 'of': 79,\n",
       " 'act': 80,\n",
       " 'lay': 81,\n",
       " 'play': 82,\n",
       " 'can': 83,\n",
       " 'many': 84,\n",
       " 'apply': 85,\n",
       " 'skill': 86,\n",
       " 'music': 87,\n",
       " 'she': 88,\n",
       " 'mr': 89,\n",
       " 'trade': 90,\n",
       " 'spend': 91,\n",
       " 'buy': 92,\n",
       " 'anyone': 93,\n",
       " 'beyond': 94,\n",
       " 'type': 95,\n",
       " 'age': 96,\n",
       " 'him': 97,\n",
       " 'safe': 98,\n",
       " 'first': 99,\n",
       " 'case': 100,\n",
       " 'hear': 101,\n",
       " 'gun': 102,\n",
       " 'teach': 103,\n",
       " 'thought': 104,\n",
       " 'meet': 105,\n",
       " 'matter': 106,\n",
       " 'follow': 107,\n",
       " 'they': 108,\n",
       " 'deal': 109,\n",
       " 'drive': 110,\n",
       " 'gas': 111,\n",
       " 'person': 112,\n",
       " 'religious': 113,\n",
       " 'concern': 114,\n",
       " 'head': 115,\n",
       " 'someone': 116,\n",
       " 'book': 117,\n",
       " 'none': 118,\n",
       " 'cost': 119,\n",
       " 'white': 120,\n",
       " 'nor': 121,\n",
       " 'ready': 122,\n",
       " 'usually': 123,\n",
       " 'night': 124,\n",
       " 'in': 125,\n",
       " 'one': 126,\n",
       " 'worker': 127,\n",
       " 'hit': 128,\n",
       " 'any': 129,\n",
       " 'seat': 130,\n",
       " 'similar': 131,\n",
       " 'staff': 132,\n",
       " 'both': 133,\n",
       " 'rock': 134,\n",
       " 'image': 135,\n",
       " 'laugh': 136,\n",
       " 'why': 137,\n",
       " 'data': 138,\n",
       " 'else': 139,\n",
       " 'mother': 140,\n",
       " 'war': 141,\n",
       " 'affect': 142,\n",
       " 'whole': 143,\n",
       " 'include': 144,\n",
       " 'though': 145,\n",
       " 'relate': 146,\n",
       " 'he': 147,\n",
       " 'somebody': 148,\n",
       " 'no': 149,\n",
       " 'fly': 150,\n",
       " 'player': 151,\n",
       " 'local': 152,\n",
       " 'movie': 153,\n",
       " 'body': 154,\n",
       " 'officer': 155,\n",
       " 'top': 156,\n",
       " 'those': 157,\n",
       " 'various': 158,\n",
       " 'year': 159,\n",
       " 'so': 160,\n",
       " 'purpose': 161,\n",
       " 'sell': 162,\n",
       " 'work': 163,\n",
       " 'finish': 164,\n",
       " 'since': 165,\n",
       " 'day': 166,\n",
       " 'until': 167,\n",
       " 'our': 168,\n",
       " 'public': 169,\n",
       " 'money': 170,\n",
       " 'just': 171,\n",
       " 'here': 172,\n",
       " 'it': 173,\n",
       " 'down': 174,\n",
       " 'sea': 175,\n",
       " 'ground': 176,\n",
       " 'hot': 177,\n",
       " 'receive': 178,\n",
       " 'better': 179,\n",
       " 'fine': 180,\n",
       " 'choose': 181,\n",
       " 'account': 182,\n",
       " 'region': 183,\n",
       " 'allow': 184,\n",
       " 'doctor': 185,\n",
       " 'true': 186,\n",
       " 'support': 187,\n",
       " 'who': 188,\n",
       " 'pay': 189,\n",
       " 'rich': 190,\n",
       " 'thus': 191,\n",
       " 'floor': 192,\n",
       " 'cold': 193,\n",
       " 'i': 194,\n",
       " 'mind': 195,\n",
       " 'grow': 196,\n",
       " 'necessary': 197,\n",
       " 'early': 198,\n",
       " 'fund': 199,\n",
       " 'tough': 200,\n",
       " 'season': 201,\n",
       " 'drug': 202,\n",
       " 'dinner': 203,\n",
       " 'guess': 204,\n",
       " 'fish': 205,\n",
       " 'tv': 206,\n",
       " 'huge': 207,\n",
       " 'face': 208,\n",
       " 'wonder': 209,\n",
       " 'common': 210,\n",
       " 'size': 211,\n",
       " 'line': 212,\n",
       " 'station': 213,\n",
       " 'worry': 214,\n",
       " 'professor': 215,\n",
       " 'catch': 216,\n",
       " 'along': 217,\n",
       " 'husband': 218,\n",
       " 'already': 219,\n",
       " 'ok': 220,\n",
       " 'woman': 221,\n",
       " 'color': 222,\n",
       " 'impact': 223,\n",
       " 'when': 224,\n",
       " 'there': 225,\n",
       " 'car': 226,\n",
       " 'democrat': 227,\n",
       " 'eight': 228,\n",
       " 'such': 229,\n",
       " 'mean': 230,\n",
       " 'stage': 231,\n",
       " 'to': 232,\n",
       " 'imagine': 233,\n",
       " 'much': 234,\n",
       " 'whose': 235,\n",
       " 'debate': 236,\n",
       " 'loss': 237,\n",
       " 'threat': 238,\n",
       " 'baby': 239,\n",
       " 'sit': 240,\n",
       " 'radio': 241,\n",
       " 'be': 242,\n",
       " 'assume': 243,\n",
       " 'add': 244,\n",
       " 'return': 245,\n",
       " 'right': 246,\n",
       " 'skin': 247,\n",
       " 'enjoy': 248,\n",
       " 'final': 249,\n",
       " 'born': 250,\n",
       " 'remain': 251,\n",
       " 'occur': 252,\n",
       " 'these': 253,\n",
       " 'soon': 254,\n",
       " 'exactly': 255,\n",
       " 'care': 256,\n",
       " 'name': 257,\n",
       " 'available': 258,\n",
       " 'red': 259,\n",
       " 'hard': 260,\n",
       " 'economic': 261,\n",
       " 'more': 262,\n",
       " 'full': 263,\n",
       " 'job': 264,\n",
       " 'box': 265,\n",
       " 'girl': 266,\n",
       " 'away': 267,\n",
       " 'left': 268,\n",
       " 'west': 269,\n",
       " 'than': 270,\n",
       " 'certain': 271,\n",
       " 'force': 272,\n",
       " 'voice': 273,\n",
       " 'stock': 274,\n",
       " 'house': 275,\n",
       " 'travel': 276,\n",
       " 'today': 277,\n",
       " 'third': 278,\n",
       " 'surface': 279,\n",
       " 'memory': 280,\n",
       " 'source': 281,\n",
       " 'everything': 282,\n",
       " 'long': 283,\n",
       " 'seek': 284,\n",
       " 'claim': 285,\n",
       " 'forward': 286,\n",
       " 'wind': 287,\n",
       " 'institution': 288,\n",
       " 'fast': 289,\n",
       " 'get': 290,\n",
       " 'sign': 291,\n",
       " 'see': 292,\n",
       " 'old': 293,\n",
       " 'same': 294,\n",
       " 'maybe': 295,\n",
       " 'use': 296,\n",
       " 'great': 297,\n",
       " 'share': 298,\n",
       " 'bit': 299,\n",
       " 'month': 300,\n",
       " 'wear': 301,\n",
       " 'rise': 302,\n",
       " 'nature': 303,\n",
       " 'represent': 304,\n",
       " 'about': 305,\n",
       " 'make': 306,\n",
       " 'side': 307,\n",
       " 'stop': 308,\n",
       " 'send': 309,\n",
       " 'simply': 310,\n",
       " 'your': 311,\n",
       " 'agency': 312,\n",
       " 'film': 313,\n",
       " 'run': 314,\n",
       " 'inside': 315,\n",
       " 'thousand': 316,\n",
       " 'possible': 317,\n",
       " 'citizen': 318,\n",
       " 'guy': 319,\n",
       " 'indeed': 320,\n",
       " 'culture': 321,\n",
       " 'cell': 322,\n",
       " 'art': 323,\n",
       " 'collection': 324,\n",
       " 'blue': 325,\n",
       " 'address': 326,\n",
       " 'physical': 327,\n",
       " 'material': 328,\n",
       " 'also': 329,\n",
       " 'suddenly': 330,\n",
       " 'paper': 331,\n",
       " 'suggest': 332,\n",
       " 'leader': 333,\n",
       " 'two': 334,\n",
       " 'build': 335,\n",
       " 'picture': 336,\n",
       " 'situation': 337,\n",
       " 'school': 338,\n",
       " 'remember': 339,\n",
       " 'whether': 340,\n",
       " 'sport': 341,\n",
       " 'subject': 342,\n",
       " 'little': 343,\n",
       " 'series': 344,\n",
       " 'trial': 345,\n",
       " 'future': 346,\n",
       " 'daughter': 347,\n",
       " 'vote': 348,\n",
       " 'study': 349,\n",
       " 'million': 350,\n",
       " 'dream': 351,\n",
       " 'change': 352,\n",
       " 'argue': 353,\n",
       " 'mouth': 354,\n",
       " 'price': 355,\n",
       " 'land': 356,\n",
       " 'attorney': 357,\n",
       " 'own': 358,\n",
       " 'practice': 359,\n",
       " 'court': 360,\n",
       " 'moment': 361,\n",
       " 'style': 362,\n",
       " 'adult': 363,\n",
       " 'ahead': 364,\n",
       " 'security': 365,\n",
       " 'point': 366,\n",
       " 'sing': 367,\n",
       " 'nation': 368,\n",
       " 'this': 369,\n",
       " 'energy': 370,\n",
       " 'something': 371,\n",
       " 'reveal': 372,\n",
       " 'call': 373,\n",
       " 'her': 374,\n",
       " 'shoulder': 375,\n",
       " 'take': 376,\n",
       " 'lead': 377,\n",
       " 'save': 378,\n",
       " 'myself': 379,\n",
       " 'lawyer': 380,\n",
       " 'live': 381,\n",
       " 'key': 382,\n",
       " 'across': 383,\n",
       " 'within': 384,\n",
       " 'six': 385,\n",
       " 'pressure': 386,\n",
       " 'sound': 387,\n",
       " 'firm': 388,\n",
       " 'as': 389,\n",
       " 'quite': 390,\n",
       " 'human': 391,\n",
       " 'phone': 392,\n",
       " 'sure': 393,\n",
       " 'most': 394,\n",
       " 'explain': 395,\n",
       " 'experience': 396,\n",
       " 'carry': 397,\n",
       " 'trouble': 398,\n",
       " 'main': 399,\n",
       " 'view': 400,\n",
       " 'how': 401,\n",
       " 'not': 402,\n",
       " 'involve': 403,\n",
       " 'item': 404,\n",
       " 'song': 405,\n",
       " 'area': 406,\n",
       " 'seven': 407,\n",
       " 'painting': 408,\n",
       " 'ago': 409,\n",
       " 'news': 410,\n",
       " 'word': 411,\n",
       " 'above': 412,\n",
       " 'score': 413,\n",
       " 'rule': 414,\n",
       " 'child': 415,\n",
       " 'wait': 416,\n",
       " 'instead': 417,\n",
       " 'establish': 418,\n",
       " 'truth': 419,\n",
       " 'pass': 420,\n",
       " 'maintain': 421,\n",
       " 'know': 422,\n",
       " 'consider': 423,\n",
       " 'leave': 424,\n",
       " 'management': 425,\n",
       " 'technology': 426,\n",
       " 'herself': 427,\n",
       " 'their': 428,\n",
       " 'current': 429,\n",
       " 'defense': 430,\n",
       " 'rather': 431,\n",
       " 'issue': 432,\n",
       " 'while': 433,\n",
       " 'under': 434,\n",
       " 'low': 435,\n",
       " 'reflect': 436,\n",
       " 'we': 437,\n",
       " 'course': 438,\n",
       " 'detail': 439,\n",
       " 'per': 440,\n",
       " 'beat': 441,\n",
       " 'policy': 442,\n",
       " 'over': 443,\n",
       " 'some': 444,\n",
       " 'edge': 445,\n",
       " 'movement': 446,\n",
       " 'heavy': 447,\n",
       " 'member': 448,\n",
       " 'forget': 449,\n",
       " 'mrs': 450,\n",
       " 'state': 451,\n",
       " 'out': 452,\n",
       " 'help': 453,\n",
       " 'chair': 454,\n",
       " 'serious': 455,\n",
       " 'later': 456,\n",
       " 'nothing': 457,\n",
       " 'hope': 458,\n",
       " 'team': 459,\n",
       " 'before': 460,\n",
       " 'prove': 461,\n",
       " 'tax': 462,\n",
       " 'result': 463,\n",
       " 'heart': 464,\n",
       " 'star': 465,\n",
       " 'prevent': 466,\n",
       " 'where': 467,\n",
       " 'law': 468,\n",
       " 'raise': 469,\n",
       " 'place': 470,\n",
       " 'because': 471,\n",
       " 'pull': 472,\n",
       " 'around': 473,\n",
       " 'ball': 474,\n",
       " 'owner': 475,\n",
       " 'strong': 476,\n",
       " 'walk': 477,\n",
       " 'major': 478,\n",
       " 'black': 479,\n",
       " 'garden': 480,\n",
       " 'operation': 481,\n",
       " 'present': 482,\n",
       " 'bar': 483,\n",
       " 'industry': 484,\n",
       " 'space': 485,\n",
       " 'teacher': 486,\n",
       " 'once': 487,\n",
       " 'writer': 488,\n",
       " 'american': 489,\n",
       " 'summer': 490,\n",
       " 'bed': 491,\n",
       " 'single': 492,\n",
       " 'several': 493,\n",
       " 'research': 494,\n",
       " 'will': 495,\n",
       " 'them': 496,\n",
       " 'respond': 497,\n",
       " 'off': 498,\n",
       " 'charge': 499,\n",
       " 'senior': 500,\n",
       " 'partner': 501,\n",
       " 'wall': 502,\n",
       " 'without': 503,\n",
       " 'card': 504,\n",
       " 'role': 505,\n",
       " 'yeah': 506,\n",
       " 'service': 507,\n",
       " 'say': 508,\n",
       " 'answer': 509,\n",
       " 'enough': 510,\n",
       " 'foot': 511,\n",
       " 'foreign': 512,\n",
       " 'wife': 513,\n",
       " 'room': 514,\n",
       " 'every': 515,\n",
       " 'process': 516,\n",
       " 'again': 517,\n",
       " 'would': 518,\n",
       " 'front': 519,\n",
       " 'window': 520,\n",
       " 'hair': 521,\n",
       " 'must': 522,\n",
       " 'clear': 523,\n",
       " 'growth': 524,\n",
       " 'what': 525,\n",
       " 'evidence': 526,\n",
       " 'business': 527,\n",
       " 'value': 528,\n",
       " 'language': 529,\n",
       " 'never': 530,\n",
       " 'attack': 531,\n",
       " 'watch': 532,\n",
       " 'trip': 533,\n",
       " 'fear': 534,\n",
       " 'through': 535,\n",
       " 'property': 536,\n",
       " 'project': 537,\n",
       " 'give': 538,\n",
       " 'its': 539,\n",
       " 'specific': 540,\n",
       " 'general': 541,\n",
       " 'site': 542,\n",
       " 'part': 543,\n",
       " 'read': 544,\n",
       " 'wide': 545,\n",
       " 'second': 546,\n",
       " 'table': 547,\n",
       " 'should': 548,\n",
       " 'perform': 549,\n",
       " 'manage': 550,\n",
       " 'election': 551,\n",
       " 'put': 552,\n",
       " 'itself': 553,\n",
       " 'interesting': 554,\n",
       " 'camera': 555,\n",
       " 'still': 556,\n",
       " 'me': 557,\n",
       " 'message': 558,\n",
       " 'throughout': 559,\n",
       " 'character': 560,\n",
       " 'figure': 561,\n",
       " 'almost': 562,\n",
       " 'think': 563,\n",
       " 'mention': 564,\n",
       " 'with': 565,\n",
       " 'machine': 566,\n",
       " 'for': 567,\n",
       " 'among': 568,\n",
       " 'notice': 569,\n",
       " 'lose': 570,\n",
       " 'bring': 571,\n",
       " 'leg': 572,\n",
       " 'after': 573,\n",
       " 'section': 574,\n",
       " 'throw': 575,\n",
       " 'amount': 576,\n",
       " 'treat': 577,\n",
       " 'begin': 578,\n",
       " 'step': 579,\n",
       " 'special': 580,\n",
       " 'term': 581,\n",
       " 'position': 582,\n",
       " 'magazine': 583,\n",
       " 'class': 584,\n",
       " 'cut': 585,\n",
       " 'deep': 586,\n",
       " 'tonight': 587,\n",
       " 'good': 588,\n",
       " 'late': 589,\n",
       " 'probably': 590,\n",
       " 'knowledge': 591,\n",
       " 'effort': 592,\n",
       " 'cover': 593,\n",
       " 'check': 594,\n",
       " 'bad': 595,\n",
       " 'hotel': 596,\n",
       " 'water': 597,\n",
       " 'page': 598,\n",
       " 'blood': 599,\n",
       " 'which': 600,\n",
       " 'exist': 601,\n",
       " 'prepare': 602,\n",
       " 'bank': 603,\n",
       " 'stuff': 604,\n",
       " 'kind': 605,\n",
       " 'central': 606,\n",
       " 'investment': 607,\n",
       " 'talk': 608,\n",
       " 'other': 609,\n",
       " 'best': 610,\n",
       " 'sister': 611,\n",
       " 'only': 612,\n",
       " 'field': 613,\n",
       " 'us': 614,\n",
       " 'light': 615,\n",
       " 'seem': 616,\n",
       " 'national': 617,\n",
       " 'market': 618,\n",
       " 'individual': 619,\n",
       " 'quality': 620,\n",
       " 'city': 621,\n",
       " 'yourself': 622,\n",
       " 'dog': 623,\n",
       " 'discuss': 624,\n",
       " 'effect': 625,\n",
       " 'soldier': 626,\n",
       " 'natural': 627,\n",
       " 'health': 628,\n",
       " 'program': 629,\n",
       " 'finally': 630,\n",
       " 'yes': 631,\n",
       " 'power': 632,\n",
       " 'draw': 633,\n",
       " 'model': 634,\n",
       " 'attention': 635,\n",
       " 'media': 636,\n",
       " 'listen': 637,\n",
       " 'up': 638,\n",
       " 'action': 639,\n",
       " 'eat': 640,\n",
       " 'develop': 641,\n",
       " 'career': 642,\n",
       " 'father': 643,\n",
       " 'thing': 644,\n",
       " 'new': 645,\n",
       " 'resource': 646,\n",
       " 'week': 647,\n",
       " 'keep': 648,\n",
       " 'could': 649,\n",
       " 'realize': 650,\n",
       " 'find': 651,\n",
       " 'easy': 652,\n",
       " 'office': 653,\n",
       " 'feeling': 654,\n",
       " 'church': 655,\n",
       " 'indicate': 656,\n",
       " 'social': 657,\n",
       " 'history': 658,\n",
       " 'chance': 659,\n",
       " 'others': 660,\n",
       " 'form': 661,\n",
       " 'glass': 662,\n",
       " 'authority': 663,\n",
       " 'coach': 664,\n",
       " 'couple': 665,\n",
       " 'look': 666,\n",
       " 'nearly': 667,\n",
       " 'smile': 668,\n",
       " 'arrive': 669,\n",
       " 'range': 670,\n",
       " 'performance': 671,\n",
       " 'record': 672,\n",
       " 'story': 673,\n",
       " 'upon': 674,\n",
       " 'option': 675,\n",
       " 'audience': 676,\n",
       " 'traditional': 677,\n",
       " 'stand': 678,\n",
       " 'decide': 679,\n",
       " 'behavior': 680,\n",
       " 'big': 681,\n",
       " 'against': 682,\n",
       " 'society': 683,\n",
       " 'successful': 684,\n",
       " 'protect': 685,\n",
       " 'level': 686,\n",
       " 'important': 687,\n",
       " 'author': 688,\n",
       " 'time': 689,\n",
       " 'poor': 690,\n",
       " 'become': 691,\n",
       " 'shake': 692,\n",
       " 'three': 693,\n",
       " 'bill': 694,\n",
       " 'country': 695,\n",
       " 'half': 696,\n",
       " 'mission': 697,\n",
       " 'student': 698,\n",
       " 'fight': 699,\n",
       " 'rate': 700,\n",
       " 'political': 701,\n",
       " 'close': 702,\n",
       " 'police': 703,\n",
       " 'note': 704,\n",
       " 'artist': 705,\n",
       " 'people': 706,\n",
       " 'fill': 707,\n",
       " 'product': 708,\n",
       " 'theory': 709,\n",
       " 'end': 710,\n",
       " 'determine': 711,\n",
       " 'population': 712,\n",
       " 'even': 713,\n",
       " 'question': 714,\n",
       " 'street': 715,\n",
       " 'you': 716,\n",
       " 'goal': 717,\n",
       " 'appear': 718,\n",
       " 'interview': 719,\n",
       " 'method': 720,\n",
       " 'production': 721,\n",
       " 'factor': 722,\n",
       " 'able': 723,\n",
       " 'weight': 724,\n",
       " 'least': 725,\n",
       " 'former': 726,\n",
       " 'southern': 727,\n",
       " 'customer': 728,\n",
       " 'may': 729,\n",
       " 'speak': 730,\n",
       " 'eye': 731,\n",
       " 'community': 732,\n",
       " 'reality': 733,\n",
       " 'sort': 734,\n",
       " 'open': 735,\n",
       " 'into': 736,\n",
       " 'another': 737,\n",
       " 'arm': 738,\n",
       " 'by': 739,\n",
       " 'control': 740,\n",
       " 'total': 741,\n",
       " 'together': 742,\n",
       " 'high': 743,\n",
       " 'food': 744,\n",
       " 'themselves': 745,\n",
       " 'increase': 746,\n",
       " 'avoid': 747,\n",
       " 'challenge': 748,\n",
       " 'recently': 749,\n",
       " 'company': 750,\n",
       " 'western': 751,\n",
       " 'building': 752,\n",
       " 'family': 753,\n",
       " 'difficult': 754,\n",
       " 'from': 755,\n",
       " 'tend': 756,\n",
       " 'generation': 757,\n",
       " 'enter': 758,\n",
       " 'less': 759,\n",
       " 'marriage': 760,\n",
       " 'thank': 761,\n",
       " 'strategy': 762,\n",
       " 'life': 763,\n",
       " 'entire': 764,\n",
       " 'evening': 765,\n",
       " 'agent': 766,\n",
       " 'hospital': 767,\n",
       " 'scene': 768,\n",
       " 'and': 769,\n",
       " 'require': 770,\n",
       " 'create': 771,\n",
       " 'morning': 772,\n",
       " 'between': 773,\n",
       " 'learn': 774,\n",
       " 'congress': 775,\n",
       " 'direction': 776,\n",
       " 'letter': 777,\n",
       " 'article': 778,\n",
       " 'start': 779,\n",
       " 'do': 780,\n",
       " 'approach': 781,\n",
       " 'might': 782,\n",
       " 'simple': 783,\n",
       " 'consumer': 784,\n",
       " 'system': 785,\n",
       " 'plant': 786,\n",
       " 'politics': 787,\n",
       " 'party': 788,\n",
       " 'likely': 789,\n",
       " 'town': 790,\n",
       " 'offer': 791,\n",
       " 'risk': 792,\n",
       " 'president': 793,\n",
       " 'agreement': 794,\n",
       " 'sometimes': 795,\n",
       " 'green': 796,\n",
       " 'everybody': 797,\n",
       " 'relationship': 798,\n",
       " 'newspaper': 799,\n",
       " 'reason': 800,\n",
       " 'example': 801,\n",
       " 'manager': 802,\n",
       " 'scientist': 803,\n",
       " 'test': 804,\n",
       " 'professional': 805,\n",
       " 'boy': 806,\n",
       " 'happen': 807,\n",
       " 'young': 808,\n",
       " 'happy': 809,\n",
       " 'toward': 810,\n",
       " 'college': 811,\n",
       " 'difference': 812,\n",
       " 'perhaps': 813,\n",
       " 'behind': 814,\n",
       " 'hand': 815,\n",
       " 'despite': 816,\n",
       " 'board': 817,\n",
       " 'government': 818,\n",
       " 'number': 819,\n",
       " 'environment': 820,\n",
       " 'north': 821,\n",
       " 'need': 822,\n",
       " 'according': 823,\n",
       " 'each': 824,\n",
       " 'middle': 825,\n",
       " 'significant': 826,\n",
       " 'meeting': 827,\n",
       " 'condition': 828,\n",
       " 'conference': 829,\n",
       " 'write': 830,\n",
       " 'the': 831,\n",
       " 'identify': 832,\n",
       " 'improve': 833,\n",
       " 'decade': 834,\n",
       " 'majority': 835,\n",
       " 'actually': 836,\n",
       " 'structure': 837,\n",
       " 'peace': 838,\n",
       " 'commercial': 839,\n",
       " 'opportunity': 840,\n",
       " 'network': 841,\n",
       " 'capital': 842,\n",
       " 'event': 843,\n",
       " 'although': 844,\n",
       " 'well': 845,\n",
       " 'door': 846,\n",
       " 'expert': 847,\n",
       " 'pick': 848,\n",
       " 'small': 849,\n",
       " 'reduce': 850,\n",
       " 'choice': 851,\n",
       " 'billion': 852,\n",
       " 'dark': 853,\n",
       " 'pretty': 854,\n",
       " 'measure': 855,\n",
       " 'animal': 856,\n",
       " 'move': 857,\n",
       " 'degree': 858,\n",
       " 'television': 859,\n",
       " 'century': 860,\n",
       " 'clearly': 861,\n",
       " 'federal': 862,\n",
       " 'positive': 863,\n",
       " 'discover': 864,\n",
       " 'however': 865,\n",
       " 'parent': 866,\n",
       " 'large': 867,\n",
       " 'discussion': 868,\n",
       " 'visit': 869,\n",
       " 'standard': 870,\n",
       " 'piece': 871,\n",
       " 'modern': 872,\n",
       " 'different': 873,\n",
       " 'outside': 874,\n",
       " 'computer': 875,\n",
       " 'cause': 876,\n",
       " 'world': 877,\n",
       " 'financial': 878,\n",
       " 'quickly': 879,\n",
       " 'interest': 880,\n",
       " 'himself': 881,\n",
       " 'provide': 882,\n",
       " 'republican': 883,\n",
       " 'produce': 884,\n",
       " 'cultural': 885,\n",
       " 'hour': 886,\n",
       " 'either': 887,\n",
       " 'campaign': 888,\n",
       " 'son': 889,\n",
       " 'minute': 890,\n",
       " 'statement': 891,\n",
       " 'during': 892,\n",
       " 'everyone': 893,\n",
       " 'fire': 894,\n",
       " 'kitchen': 895,\n",
       " 'crime': 896,\n",
       " 'popular': 897,\n",
       " 'certainly': 898,\n",
       " 'onto': 899,\n",
       " 'far': 900,\n",
       " 'democratic': 901,\n",
       " 'ever': 902,\n",
       " 'development': 903,\n",
       " 'including': 904,\n",
       " 'science': 905,\n",
       " 'candidate': 906,\n",
       " 'particular': 907,\n",
       " 'back': 908,\n",
       " 'nice': 909,\n",
       " 'executive': 910,\n",
       " 'spring': 911,\n",
       " 'yard': 912,\n",
       " 'always': 913,\n",
       " 'past': 914,\n",
       " 'believe': 915,\n",
       " 'ability': 916,\n",
       " 'report': 917,\n",
       " 'response': 918,\n",
       " 'plan': 919,\n",
       " 'compare': 920,\n",
       " 'medical': 921,\n",
       " 'success': 922,\n",
       " 'employee': 923,\n",
       " 'group': 924,\n",
       " 'understand': 925,\n",
       " 'training': 926,\n",
       " 'contain': 927,\n",
       " 'official': 928,\n",
       " 'budget': 929,\n",
       " 'activity': 930,\n",
       " 'serve': 931,\n",
       " 'feel': 932,\n",
       " 'participant': 933,\n",
       " 'ask': 934,\n",
       " 'pattern': 935,\n",
       " 'international': 936,\n",
       " 'treatment': 937,\n",
       " 'expect': 938,\n",
       " 'near': 939,\n",
       " 'very': 940,\n",
       " 'decision': 941,\n",
       " 'recent': 942,\n",
       " 'fact': 943,\n",
       " 'education': 944,\n",
       " 'especially': 945,\n",
       " 'turn': 946,\n",
       " 'design': 947,\n",
       " 'suffer': 948,\n",
       " 'information': 949,\n",
       " 'economy': 950,\n",
       " 'personal': 951,\n",
       " 'base': 952,\n",
       " 'brother': 953,\n",
       " 'continue': 954,\n",
       " 'analysis': 955,\n",
       " 'benefit': 956,\n",
       " 'anything': 957,\n",
       " 'whatever': 958,\n",
       " 'director': 959,\n",
       " 'hundred': 960,\n",
       " 'often': 961,\n",
       " 'particularly': 962,\n",
       " 'beautiful': 963,\n",
       " 'friend': 964,\n",
       " 'military': 965,\n",
       " 'store': 966,\n",
       " 'recognize': 967,\n",
       " 'responsibility': 968,\n",
       " 'environmental': 969,\n",
       " 'but': 970}"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer1.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "570538aa-aa03-464c-8251-761db4194a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.saving.save_model(model, '../models/model_v1.keras', overwrite=True)\n",
    "# loaded_model = keras.saving.load_model(\"model1.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "cf77c2cb-3610-4558-9190-d1388ece3edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 92ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.53092307],\n",
       "       [0.53092664],\n",
       "       [0.5309242 ],\n",
       "       ...,\n",
       "       [0.53092647],\n",
       "       [0.5309263 ],\n",
       "       [0.5309273 ]], dtype=float32)"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict([query_padded, likes_padded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a194c6f1-1e76-4c27-a4b0-3dabec35d5e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1848cd38-fa13-42cb-bbb3-fcde1e27b918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep tonight indeed war per.\n",
      "28 28\n",
      "Wonder common.\n",
      "14 14\n",
      "Anyone people cell fill.\n",
      "24 24\n",
      "Collection theory beat.\n",
      "23 23\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    q = data['query'][i]\n",
    "    print(q)\n",
    "    seq = tokenizer.texts_to_sequences(q)\n",
    "    pad = pad_sequences(seq)\n",
    "    print(len(seq), len(pad))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "2ab9beb5-594b-4638-9fde-529e68a68c8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[186], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mfit_on_texts(queries \u001b[38;5;241m+\u001b[39m likes)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Convert text to sequences\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m queries_seq \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences(queries, maxlen \u001b[38;5;241m=\u001b[39m \u001b[43mparams\u001b[49m)\n\u001b[1;32m     16\u001b[0m likes_seq \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences(likes, maxlen \u001b[38;5;241m=\u001b[39m params)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Pad sequences\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'params' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Example data\n",
    "queries = [\"best smartphones 2024\", \"latest fashion trends\"]\n",
    "likes = [\"smartphones, technology, gadgets\", \"fashion, style, clothing\"]\n",
    "labels = [1, 0] # 1 for relevant, 0 for not relevant\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(queries + likes)\n",
    "\n",
    "# Convert text to sequences\n",
    "queries_seq = tokenizer.texts_to_sequences(queries, maxlen = params)\n",
    "likes_seq = tokenizer.texts_to_sequences(likes, maxlen = params)\n",
    "\n",
    "# Pad sequences\n",
    "queries_seq = pad_sequences(queries_seq, padding='post')\n",
    "likes_seq = pad_sequences(likes_seq, padding='post')\n",
    "\n",
    "# Convert sequences to tensors\n",
    "queries_tensor = tf.convert_to_tensor(queries_seq)\n",
    "likes_tensor = tf.convert_to_tensor(likes_seq)\n",
    "labels_tensor = tf.convert_to_tensor(labels)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128, input_length=queries_seq.shape[1]),\n",
    "    SimpleRNN(128),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Example: Predicting the priority score of a new search query\n",
    "new_search_query = \"best smartphones 2024\"\n",
    "new_past_searches = \"smartphones, technology\"\n",
    "\n",
    "# Preprocess the new data\n",
    "new_combined_text = new_search_query + ' ' + new_past_searches\n",
    "new_sequence = tokenizer.texts_to_sequences([new_combined_text])\n",
    "new_padded_sequence = pad_sequences(new_sequence, padding='post')\n",
    "\n",
    "# Convert to tensor\n",
    "new_X = tf.convert_to_tensor(new_padded_sequence)\n",
    "\n",
    "# Predict\n",
    "predicted_priority = model.predict(new_X)\n",
    "\n",
    "# Convert back to the original priority score range\n",
    "predicted_priority = (predicted_priority * (data['priority_score'].max() - data['priority_score'].min())) + data['priority_score'].min()\n",
    "\n",
    "print(f\"Predicted priority score: {predicted_priority[0][0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "6d31dca6-aa1f-499e-97b4-2be727416784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 1000)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "7a4fee5f-920c-4701-bd47-38ecea153401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1000) (1000, 1000)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Concatenate, SimpleRNN, Dense\n",
    "\n",
    "# Define the model architecture\n",
    "query_input = Input(shape=(None,), dtype='int32')\n",
    "likes_input = Input(shape=(None,), dtype='int32')\n",
    "\n",
    "# Embedding layers for both inputs\n",
    "query_embedding = Embedding(input_dim=params, output_dim=128)(query_input)\n",
    "likes_embedding = Embedding(input_dim=params, output_dim=128)(likes_input)\n",
    "\n",
    "# Concatenate the embeddings\n",
    "merged = Concatenate()([query_embedding, likes_embedding])\n",
    "\n",
    "# Simple RNN layer\n",
    "rnn_layer = SimpleRNN(128)(merged)\n",
    "\n",
    "# Dense layer for regression\n",
    "output = Dense(1)(rnn_layer)\n",
    "\n",
    "# Compile the model\n",
    "model = Model(inputs=[query_input, likes_input], outputs=output)\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "print(query_padded.shape, likes_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f3439d-111a-45f1-afce-0dca00c10e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "model.fit([query_padded, likes_padded], data['priority_score'], epochs=20, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d4c0a26-05c4-4830-abee-079f51a6a29c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Preprocess the new data\u001b[39;00m\n\u001b[1;32m      6\u001b[0m new_combined_text \u001b[38;5;241m=\u001b[39m new_search_query \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m new_past_searches\n\u001b[0;32m----> 7\u001b[0m new_sequence \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mtexts_to_sequences([new_combined_text])\n\u001b[1;32m      8\u001b[0m new_padded_sequence \u001b[38;5;241m=\u001b[39m pad_sequences(new_sequence, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Convert to tensor\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d6fdc8-14be-4fd3-8f60-4b43e0a779b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a68506cd-1a02-43a9-b9b0-e03969048b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(['sleep', 'hello', 'good'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fdabb011-9708-4aaa-9c6e-ff6aeca5ffc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"class_name\": \"Tokenizer\", \"config\": {\"num_words\": 10000, \"filters\": \"!\\\\\"#$%&()*+,-./:;<=>?@[\\\\\\\\]^_`{|}~\\\\t\\\\n\", \"lower\": true, \"split\": \" \", \"char_level\": false, \"oov_token\": \"<OOV>\", \"document_count\": 3, \"word_counts\": \"{\\\\\"sleep\\\\\": 1, \\\\\"hello\\\\\": 1, \\\\\"good\\\\\": 1}\", \"word_docs\": \"{\\\\\"sleep\\\\\": 1, \\\\\"hello\\\\\": 1, \\\\\"good\\\\\": 1}\", \"index_docs\": \"{\\\\\"2\\\\\": 1, \\\\\"3\\\\\": 1, \\\\\"4\\\\\": 1}\", \"index_word\": \"{\\\\\"1\\\\\": \\\\\"<OOV>\\\\\", \\\\\"2\\\\\": \\\\\"sleep\\\\\", \\\\\"3\\\\\": \\\\\"hello\\\\\", \\\\\"4\\\\\": \\\\\"good\\\\\"}\", \"word_index\": \"{\\\\\"<OOV>\\\\\": 1, \\\\\"sleep\\\\\": 2, \\\\\"hello\\\\\": 3, \\\\\"good\\\\\": 4}\"}}'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['good'])\n",
    "tokenizer.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "747df5eb-5a84-4b20-8dc5-c5bf50f0e828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [1]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['better', 'sleeping'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329be0c5-b948-413a-9a33-e8f6e7a43f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a0f1cf4b-8dd4-499e-bcd1-7535bfa34063",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/konda.sandeep/nltk_data'\n    - '/Users/konda.sandeep/Projects/personal/major project/backend/env/nltk_data'\n    - '/Users/konda.sandeep/Projects/personal/major project/backend/env/share/nltk_data'\n    - '/Users/konda.sandeep/Projects/personal/major project/backend/env/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m search_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMachine learning techniques for data analysis\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Preprocess the search query\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m preprocessed_query \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_search_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearch_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreprocessed tokens:\u001b[39m\u001b[38;5;124m\"\u001b[39m, preprocessed_query)\n",
      "Cell \u001b[0;32mIn[53], line 14\u001b[0m, in \u001b[0;36mpreprocess_search_query\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_search_query\u001b[39m(query):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Tokenize the query\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Convert to lowercase\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Remove punctuation\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string\u001b[38;5;241m.\u001b[39mpunctuation]\n",
      "File \u001b[0;32m~/Projects/personal/major project/backend/env/lib/python3.12/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m~/Projects/personal/major project/backend/env/lib/python3.12/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/Projects/personal/major project/backend/env/lib/python3.12/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/Projects/personal/major project/backend/env/lib/python3.12/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m~/Projects/personal/major project/backend/env/lib/python3.12/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/konda.sandeep/nltk_data'\n    - '/Users/konda.sandeep/Projects/personal/major project/backend/env/nltk_data'\n    - '/Users/konda.sandeep/Projects/personal/major project/backend/env/share/nltk_data'\n    - '/Users/konda.sandeep/Projects/personal/major project/backend/env/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# !pip install nltk\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "def preprocess_search_query(query):\n",
    "    # Tokenize the query\n",
    "    tokens = word_tokenize(query.lower())  # Convert to lowercase\n",
    "    \n",
    "    # Remove punctuation\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Example search query\n",
    "search_query = \"Machine learning techniques for data analysis\"\n",
    "\n",
    "# Preprocess the search query\n",
    "preprocessed_query = preprocess_search_query(search_query)\n",
    "print(\"Preprocessed tokens:\", preprocessed_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c129e4c1-54f8-4b29-9795-afc27b0ed703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3\n",
      "Tokenized sequences: [[1], [2]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the text containing the words\n",
    "texts = [\"sleeping\", \"sleep\"]\n",
    "\n",
    "# Initialize Tokenizer\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "\n",
    "# Fit tokenizer on texts\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Tokenize the texts\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Check vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "\n",
    "# Print the tokenized sequences\n",
    "print(\"Tokenized sequences:\", sequences)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
